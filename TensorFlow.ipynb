{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2GKKP-1ZRO6V",
        "_cKd4O8pRVnl",
        "v8AmglaI4mrN",
        "Pr_FScDBmKy_",
        "Ljor6eQjK9-s"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmjxVHkET+sCwTRS/STDNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Astle-sudo/3D/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "nu77qR-tlhSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KnT1dL7chuci"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import tensorflow as flow\n",
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Linear Network**"
      ],
      "metadata": {
        "id": "2GKKP-1ZRO6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = flow.keras.Sequential([flow.keras.layers.Dense(units=1, input_shape=[1])])\n",
        "# model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# x = numpy.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "# y = numpy.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "# model.fit(x, y, epochs=500, verbose=0)\n",
        "\n",
        "x = flow.range(0,10)\n",
        "\n",
        "y = 2 * x\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "sfmLOvDSh5WL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5f3dac-6cba-49fd-cadc-679052b97267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12 14 16 18], shape=(10,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "_cKd4O8pRVnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n",
        "# !wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n",
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "id": "iOtlelOHseIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# local_zip = './horse-or-human.zip'\n",
        "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "# local_zip = './validation-horse-or-human.zip'\n",
        "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall('./validation-horse-or-human')\n",
        "\n",
        "local_zip = './cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "cwySNqTYs1CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "# train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "# validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
        "# validation_human_dir = os.path.join('./validation-horse-or-human/humans')\n",
        "\n",
        "# train_horse_names = os.listdir(train_horse_dir)\n",
        "# train_human_names = os.listdir(train_human_dir)\n",
        "# validation_horse_hames = os.listdir(validation_horse_dir)\n",
        "# validation_human_names = os.listdir(validation_human_dir)\n",
        "\n",
        "base_dir = 'cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "train_cat_fnames = os.listdir( train_cats_dir )\n",
        "train_dog_fnames = os.listdir( train_dogs_dir )"
      ],
      "metadata": {
        "id": "BG2XgID7s_th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fmist = flow.keras.datasets.fashion_mnist\n",
        "# (training_images, training_lables), (test_images, test_lables) = fmist.load_data()\n",
        "# training_images = training_images/ 255.0\n",
        "# test_images = test_images/ 255.0"
      ],
      "metadata": {
        "id": "XX4P5arjRcm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallBack (flow.keras.callbacks.Callback):\n",
        "  def on_epoch_end (self, epoch, logs={}):\n",
        "    if logs.get('accuracy') is not None and logs.get('accuracy') > 0.70:\n",
        "      self.model.stop_training = True"
      ],
      "metadata": {
        "id": "F4XorW0vbW3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = flow.keras.models.Sequential([\n",
        "    flow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    flow.keras.layers.MaxPooling2D(2, 2),\n",
        "    flow.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
        "    flow.keras.layers.MaxPooling2D(2,2),\n",
        "    flow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    flow.keras.layers.MaxPooling2D(2,2),\n",
        "    flow.keras.layers.Flatten(),\n",
        "    flow.keras.layers.Dense(512, activation='relu'),\n",
        "    flow.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer = flow.keras.optimizers.RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# model.fit (training_images, training_lables, epochs=5, callbacks=[CallBack ()], verbose=0)"
      ],
      "metadata": {
        "id": "PjZUvAE_SLvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "3YRe8tdwvJJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = flow.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        "  )\n",
        "validation_datagen = flow.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # './horse-or-human/',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir, # './validation-horse-or-human/',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "m6-invWYuUrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(train_generator,\n",
        "#                     steps_per_epoch=100,\n",
        "#                     epochs=15, verbose=2,\n",
        "#                     validation_data=val_generator,\n",
        "#                     validation_steps=50,\n",
        "#                   )"
      ],
      "metadata": {
        "id": "Ldac8XUMvGag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acc = history.history['accuracy']\n",
        "# loss = history.history['loss']\n",
        "\n",
        "# plt.plot(range(1, 16), acc)\n",
        "# # plt.plot(range(1, 16), loss)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "k-OQCqxjbaci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning**"
      ],
      "metadata": {
        "id": "v8AmglaI4mrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "metadata": {
        "id": "NYDVW5fo4rrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = flow.keras.applications.inception_v3.InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = None)\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "-gAaT0Rk5Mhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "id": "br3WOp2L5rtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_output = last_layer.output"
      ],
      "metadata": {
        "id": "VMr6lFhr6AJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = flow.keras.layers.Flatten()(last_output)\n",
        "x = flow.keras.layers.Dense(1024, activation='relu')(x)\n",
        "x = flow.keras.layers.Dropout(0.2)(x)\n",
        "x = flow.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = flow.keras.Model(pre_trained_model.input, x)\n",
        "model.compile(optimizer = flow.keras.optimizers.RMSprop(learning_rate=0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "0Scr_4Wu6Ja-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(train_generator,validation_data = validation_generator,steps_per_epoch = 100,epochs = 20,validation_steps = 50,verbose = 2)"
      ],
      "metadata": {
        "id": "WKSU9s-i6xVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recurrent Neural Networks**"
      ],
      "metadata": {
        "id": "Pr_FScDBmKy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ],
      "metadata": {
        "id": "XfgcNIhFmPtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11e077b-4319-485e-ce9d-3ceb2d27ace2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-28 21:53:57--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 2607:f8b0:4023:c06::cf, 2607:f8b0:4023:c0d::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘sarcasm.json’\n",
            "\n",
            "sarcasm.json        100%[===================>]   5.38M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-11-28 21:53:57 (66.5 MB/s) - ‘sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"./sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)"
      ],
      "metadata": {
        "id": "tfTecwWameAY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = list()\n",
        "labels = list()\n",
        "urls = list()\n",
        "\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])"
      ],
      "metadata": {
        "id": "K1w2BH1tmi3B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "max_length = 40\n",
        "embedding_dim = 16\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "metadata": {
        "id": "WRhzMI3dyPJy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = flow.keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = flow.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "labels = numpy.array(labels).reshape(len(labels),1)\n",
        "\n",
        "print(padded.shape, padded[0].shape, padded[0])"
      ],
      "metadata": {
        "id": "3wa-5Ts8mpHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = flow.keras.Sequential([\n",
        "    flow.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    flow.keras.layers.GlobalAveragePooling1D(),\n",
        "    flow.keras.layers.Dense(24, activation='relu'),\n",
        "    flow.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "a2yVBwnqxyT8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(padded, labels, epochs=num_epochs, verbose=2)"
      ],
      "metadata": {
        "id": "MjgQLr4Bx8tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683a20f2-9da4-4c37-b2f4-5f7bbfeac3b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "835/835 - 20s - loss: 0.5285 - accuracy: 0.7240 - 20s/epoch - 24ms/step\n",
            "Epoch 2/10\n",
            "835/835 - 5s - loss: 0.2967 - accuracy: 0.8791 - 5s/epoch - 6ms/step\n",
            "Epoch 3/10\n",
            "835/835 - 3s - loss: 0.2355 - accuracy: 0.9073 - 3s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "835/835 - 4s - loss: 0.1986 - accuracy: 0.9234 - 4s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "835/835 - 4s - loss: 0.1729 - accuracy: 0.9335 - 4s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "835/835 - 4s - loss: 0.1540 - accuracy: 0.9423 - 4s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "835/835 - 3s - loss: 0.1388 - accuracy: 0.9483 - 3s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "835/835 - 3s - loss: 0.1268 - accuracy: 0.9552 - 3s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "835/835 - 3s - loss: 0.1157 - accuracy: 0.9582 - 3s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "835/835 - 4s - loss: 0.1067 - accuracy: 0.9617 - 4s/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTMs**"
      ],
      "metadata": {
        "id": "Ljor6eQjK9-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 15UqmiIm0xwh9mt0IYq2z3jHaauxQSTQT"
      ],
      "metadata": {
        "id": "9j2U9scNLFEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('./irish-lyrics-eof.txt').read()\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "metadata": {
        "id": "PG9hjrCxLKxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = flow.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "TLNMYGYcLXHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = flow.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = flow.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "QRaSpSBNLsZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "lstm_units = 150\n",
        "learning_rate = 0.01\n",
        "\n",
        "print(xs[0])\n",
        "\n",
        "model = flow.keras.Sequential([\n",
        "          flow.keras.layers.Embedding(total_words, embedding_dim, input_length=max_sequence_len-1),\n",
        "          flow.keras.layers.Bidirectional(flow.keras.layers.LSTM(lstm_units)),\n",
        "          flow.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=flow.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "URFwhMFGMbc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "history = model.fit(xs, ys, epochs=epochs)"
      ],
      "metadata": {
        "id": "ePbbx7N6NPaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "J7iDt9uGNW_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = flow.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tprobabilities = model.predict(token_list, verbose=0)\n",
        "\tpredicted = numpy.argmax(probabilities, axis=-1)[0]\n",
        "\tif predicted != 0:\n",
        "\t\toutput_word = tokenizer.index_word[predicted]\n",
        "\t\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "a3QJm-mPNkbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Time Series**"
      ],
      "metadata": {
        "id": "QpZNIEObYfAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if type(series) is tuple:\n",
        "      for series_num in series:\n",
        "        plt.plot(time[start:end], series_num[start:end], format)\n",
        "    else:\n",
        "      plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xbdNk7RzYiQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trend(time, slope=0):\n",
        "    series = slope * time\n",
        "    return series\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    data_pattern = numpy.where(season_time < 0.4,numpy.cos(season_time * 2 * numpy.pi),1 / numpy.exp(3 * season_time))\n",
        "    return data_pattern\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    data_pattern = amplitude * seasonal_pattern(season_time)\n",
        "    return data_pattern\n",
        "\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    rnd = numpy.random.RandomState(seed)\n",
        "    noise = rnd.randn(len(time)) * noise_level\n",
        "    return noise"
      ],
      "metadata": {
        "id": "j8PVY_YCY0Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time = numpy.arange(4 * 365 + 1, dtype=\"float32\")\n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.05\n",
        "noise_level = 5\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "series += noise(time, noise_level, seed=42)\n",
        "plot_series(time, series)"
      ],
      "metadata": {
        "id": "wbzEgr0IZLDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_time = 1000\n",
        "\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ],
      "metadata": {
        "id": "uO5fvhk7ZUIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_forecast = series[split_time - 1:-1]\n",
        "time_step = 100"
      ],
      "metadata": {
        "id": "sDSiwCqZZiz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average_forecast(series, window_size):\n",
        "    forecast = []\n",
        "    for time in range(len(series) - window_size):\n",
        "      forecast.append(series[time:time + window_size].mean())\n",
        "    forecast = numpy.array(forecast)\n",
        "\n",
        "    return forecast"
      ],
      "metadata": {
        "id": "hpgXw6uQZ3vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n",
        "plot_series(time_valid, (x_valid, moving_avg))\n",
        "diff_series = (series[365:] - series[:-365])\n",
        "diff_time = time[365:]\n",
        "plot_series(diff_time, diff_series)\n",
        "diff_moving_avg = moving_average_forecast(diff_series, 30)\n",
        "diff_moving_avg = diff_moving_avg[split_time - 365 - 30:]\n",
        "diff_series = diff_series[split_time - 365:]\n",
        "plot_series(time_valid, (diff_series, diff_moving_avg))\n",
        "diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n",
        "plot_series(time_valid, (x_valid, diff_moving_avg_plus_past))\n",
        "diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-359], 11) + diff_moving_avg\n",
        "plot_series(time_valid, (x_valid, diff_moving_avg_plus_smooth_past))"
      ],
      "metadata": {
        "id": "KBkfjFLDZ_Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "metadata": {
        "id": "tN3X8djyOQam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    dataset = flow.data.Dataset.from_tensor_slices(series)\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6qviF8MTN8Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
      ],
      "metadata": {
        "id": "mUNZrKwYON_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_baseline = flow.keras.models.Sequential([\n",
        "#     flow.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"),\n",
        "#     flow.keras.layers.Dense(10, activation=\"relu\"),\n",
        "#     flow.keras.layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# lr_schedule = flow.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "# model_baseline.summary()\n",
        "# model_baseline.compile(loss=\"mse\", optimizer=flow.keras.optimizers.SGD(learning_rate=7e-6, momentum=0.9))\n",
        "# model_baseline.fit(dataset,epochs=10)"
      ],
      "metadata": {
        "id": "Zi9rz12bOUm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flow.keras.backend.clear_session()\n",
        "model_baseline = flow.keras.models.Sequential([\n",
        "    flow.keras.layers.Lambda(lambda x: flow.expand_dims(x, axis=-1), input_shape=[None]),\n",
        "    flow.keras.layers.Bidirectional(flow.keras.layers.LSTM(32)),\n",
        "    flow.keras.layers.Dense(1),\n",
        "    flow.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "model_baseline.compile(loss='mse', optimizer=flow.keras.optimizers.SGD(learning_rate=5e-6, momentum=0.9))\n",
        "model_baseline.fit(dataset, epochs=10, verbose=0)"
      ],
      "metadata": {
        "id": "90HD6BHBNihS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = []\n",
        "forecast_series = series[split_time - window_size:]\n",
        "for time in range(len(forecast_series) - window_size):\n",
        "  forecast.append(model_baseline.predict(forecast_series[time:time + window_size][numpy.newaxis]))\n",
        "results = numpy.array(forecast).squeeze()\n",
        "plot_series(time_valid, (x_valid, results))"
      ],
      "metadata": {
        "id": "WU2kQGIuOvPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(flow.keras.metrics.mean_squared_error(x_valid, results).numpy())\n",
        "print(flow.keras.metrics.mean_absolute_error(x_valid, results).numpy())"
      ],
      "metadata": {
        "id": "C7g6qWPMPG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# window_size = 60\n",
        "# batch_size = 32\n",
        "# trained_set = dataset\n",
        "\n",
        "# model = flow.keras.models.Sequential([\n",
        "#     flow.keras.layers.Conv1D(filters=32, kernel_size=5, padding='casual', strides=1, activation='relu', input_shape=[None, 1]),\n",
        "#     flow.keras.layers.LSTM(32, return_sequences=True),\n",
        "#     flow.keras.layers.LSTM(32),\n",
        "#     flow.keras.layers.Dense(30, activation='relu'),\n",
        "#     flow.keras.layers.Dense(10, activation='relu'),\n",
        "#     flow.keras.layers.Dense(1),\n",
        "#     flow.keras.layers.Lambda(lambda x: x * 400.0)\n",
        "# ])\n",
        "\n",
        "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n",
        "# model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n",
        "# history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ],
      "metadata": {
        "id": "RBCgRhhDjra2"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}